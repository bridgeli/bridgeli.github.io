---
title: Jsoup在简单防御XSS攻击和网络爬虫的简单应用
author: Bridge Li
type: post
date: 2014-09-30T02:00:32+00:00

duoshuo_thread_id:
  - 1.1604454626757E+18
categories:
  - Java
---
跨站攻击一直是web安全的一大问题，稍有不慎就会中招，各种防不胜防，今天在网上闲逛，发现一个第三方JAR不仅可以简单防御还可以爬取网页，所以写一篇小文以记之，也供有需要的人参考。  
预防跨站攻击代码如下：

```

@Test
public void testJsoup() {
    String unsafe = "<p><a href=&#8217;http://example.com/&#8217; onclick=&#8217;stealCookies()&#8217;>Link,<alert>0</alert></a></p>";
    String safe = Jsoup.clean(unsafe, Whitelist.basic());
    System.out.println(safe);
}

```

其中 unsafe 就是用户提交的数据，其中包含 onclick 和 alert 可能会有害，需要过滤，而 Whitelist.basic() 则是过滤规则，safe 就是过滤后的安全数据。

在网络爬虫方面，曾听北京尚学堂马士兵老师讲过一次正则表达式的简单应用：抓取网页上的邮箱地址，有很多人记不住，但有了这个第三方工具，则轻松很多，他可以根据一个URL直接去抓取，省了不少事，猜测内部也是用正则实现的。  
网络爬虫代码如下：

```

@Test
public void testSpider() {

    String url = "http://www.rijiben.com/";

    Document doc = null;
    try {
        doc = Jsoup.connect(url).get();
    } catch (IOException e) {
        e.printStackTrace();
    }
    if (null != doc) {
        Elements listrens = doc.getElementsByAttributeValue("class", "listren");
        for (Element listren : listrens) {
            String text = listren.select("li").select("a").html();
            System.out.println(text);
        }
    } else {
        System.err.println("网络出异常！");
    }
}

```

这是抓取日记本上面，历史上的今日的例子，老夫打算将来用到自己的微信公众号上面，怎么样，是不是很简单？

详细参考Jsoup的文档：http://www.open-open.com/jsoup/

**2015-06-19 补充说明：**

jsoup只能抓取普通方式生产的HTML页面，如果该页面是由ajax生成则不行，此时可以用HttpUnit，具体例子就不多说了